<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Running Ollama on Intel Arc | Dan Pawlik blog site</title>
<meta name=keywords content="first"><meta name=description content="Desc Text."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://danpawlik.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://danpawlik.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://danpawlik.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://danpawlik.github.io/apple-touch-icon.png><link rel=mask-icon href=https://danpawlik.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://danpawlik.github.io/posts/ollama/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://danpawlik.github.io/posts/ollama/"><meta property="og:site_name" content="Dan Pawlik blog site"><meta property="og:title" content="Running Ollama on Intel Arc"><meta property="og:description" content="Desc Text."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-29T16:54:04+02:00"><meta property="article:modified_time" content="2025-04-29T16:54:04+02:00"><meta property="article:tag" content="First"><meta property="og:image" content="https://danpawlik.github.io/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://danpawlik.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Running Ollama on Intel Arc"><meta name=twitter:description content="Desc Text."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://danpawlik.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Running Ollama on Intel Arc","item":"https://danpawlik.github.io/posts/ollama/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Running Ollama on Intel Arc","name":"Running Ollama on Intel Arc","description":"Desc Text.","keywords":["first"],"articleBody":"Introduction The Ollama framework is used for running and managing large languagel models (LLMs) on local computer such as laptop. As a company employee (Red Hat), I recently received a new company laptop - Lenovo ThinkPad P1G7: which have NPU (Neural Processing Unit), but no graphic card with CUDA, so it raises some complications to start working on Ollama. Let’s see if the framework can be running on my laptop and let’ try to make some exploration with available models.\nSystem details As I mentioned, my working laptop is: Lenovo P1 G7. Below screen shoot from fastfetch (neofetch is not developed anymore :( )\nIs possible to run Ollama without Intel fork? Let’s see On the beginning of that adventure, I installed the ollama using the most common installation script available on theirs page. Before that, let’ see what the script is doing\nNote: I was not aware about the main requirements for the Ollama, which is: CUDA.\nAnd after executing script…\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ~ ❯ curl -fsSL https://ollama.com/install.sh | sh ✘ 0|INT 14s 15:37:49 \u003e\u003e\u003e Cleaning up old version at /usr/local/lib/ollama \u003e\u003e\u003e Installing ollama to /usr/local \u003e\u003e\u003e Downloading Linux amd64 bundle ######################################################################## 100.0% \u003e\u003e\u003e Creating ollama user... \u003e\u003e\u003e Adding ollama user to render group... \u003e\u003e\u003e Adding ollama user to video group... \u003e\u003e\u003e Adding current user to ollama group... \u003e\u003e\u003e Creating ollama systemd service... \u003e\u003e\u003e Enabling and starting ollama service... Created symlink '/etc/systemd/system/default.target.wants/ollama.service' → '/etc/systemd/system/ollama.service'. \u003e\u003e\u003e The Ollama API is now available at 127.0.0.1:11434. \u003e\u003e\u003e Install complete. Run \"ollama\" from the command line. WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode. Eh, what’s now? For sure I need to uninstall what has been done using the guide and let’s search for another solution.\nIntel Ipex llm After quick Googling, I spotted some project on Github that might be interesting and can “fit” my requirement - it was Intel Ipex llm available here. Of course there are many projects in Google, that are possitioned higher than this one - right now AI is very hot topic.\nInstallation There is no rpm package available (I’m using Fedora 41), so I will run the binary directly after unpacking the archive.\nDownload the archive from Github project release page - Ipex LLM:\n1 curl -LO https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ollama-ipex-llm-2.3.0b20250415-ubuntu.tgz After download, unpack:\n1 tar xaf ollama-ipex-llm-2.3.0b20250415-ubuntu.tgz Then go to the directly and start the Ollama server by executing:\n1 ./ollama serve For making simply test for some programming question, I will use a codellama:13b model available here. More models with the description are available in the catalog page.\nThat’s all for that blog post. In next post I will try to show how the codellama works with simply queries.\n","wordCount":"458","inLanguage":"en","image":"https://danpawlik.github.io/%3Cimage%20path/url%3E","datePublished":"2025-04-29T16:54:04+02:00","dateModified":"2025-04-29T16:54:04+02:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://danpawlik.github.io/posts/ollama/"},"publisher":{"@type":"Organization","name":"Dan Pawlik blog site","logo":{"@type":"ImageObject","url":"https://danpawlik.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://danpawlik.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://danpawlik.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://danpawlik.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://danpawlik.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://danpawlik.github.io title="main page"><span>main page</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://danpawlik.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://danpawlik.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Running Ollama on Intel Arc</h1><div class=post-description>Desc Text.</div><div class=post-meta><span title='2025-04-29 16:54:04 +0200 CEST'>April 29, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;458 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ollama.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#system-details>System details</a></li><li><a href=#is-possible-to-run-ollama-without-intel-fork-lets-see>Is possible to run Ollama without Intel fork? Let&rsquo;s see</a></li><li><a href=#intel-ipex-llm>Intel Ipex llm</a><ul><li><a href=#installation>Installation</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>The Ollama framework is used for running and managing large languagel models (LLMs)
on local computer such as laptop.
As a company employee (Red Hat), I recently received a new company laptop - Lenovo ThinkPad P1G7:
which have NPU (Neural Processing Unit), but no graphic card with CUDA,
so it raises some complications to start working on Ollama.
Let&rsquo;s see if the framework can be running on my laptop and let&rsquo; try to make
some exploration with available models.</p><h2 id=system-details>System details<a hidden class=anchor aria-hidden=true href=#system-details>#</a></h2><p>As I mentioned, my working laptop is: Lenovo P1 G7. Below screen shoot from
fastfetch (neofetch is not developed anymore :( )</p><p><img alt=fastfetch loading=lazy src=../../posts/images/01-fastfetch.jpg></p><h2 id=is-possible-to-run-ollama-without-intel-fork-lets-see>Is possible to run Ollama without Intel fork? Let&rsquo;s see<a hidden class=anchor aria-hidden=true href=#is-possible-to-run-ollama-without-intel-fork-lets-see>#</a></h2><p>On the beginning of that adventure, I installed the ollama using the
most common installation script available on theirs <a href=https://ollama.com/download>page</a>.
Before that, let&rsquo; see what the script is <a href=https://ollama.com/install.sh>doing</a></p><p>Note: I was not aware about the main requirements for the Ollama, which is: CUDA.</p><p><img alt=ollama-install loading=lazy src=../../posts/images/02-ollama-install.jpg></p><p>And after executing script&mldr;</p><p><img alt=ollama-install loading=lazy src=../../posts/images/03-oolama-fail.jpg></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13>13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14>14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15>15</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>~ ❯ curl -fsSL https://ollama.com/install.sh <span class=p>|</span> sh                                                                                                     ✘ 0<span class=p>|</span>INT 14s 15:37:49
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Cleaning up old version at /usr/local/lib/ollama
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Installing ollama to /usr/local
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Downloading Linux amd64 bundle
</span></span><span class=line><span class=cl><span class=c1>######################################################################## 100.0%</span>
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Creating ollama user...
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Adding ollama user to render group...
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Adding ollama user to video group...
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Adding current user to ollama group...
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Creating ollama systemd service...
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Enabling and starting ollama service...
</span></span><span class=line><span class=cl>Created symlink <span class=s1>&#39;/etc/systemd/system/default.target.wants/ollama.service&#39;</span> → <span class=s1>&#39;/etc/systemd/system/ollama.service&#39;</span>.
</span></span><span class=line><span class=cl>&gt;&gt;&gt; The Ollama API is now available at 127.0.0.1:11434.
</span></span><span class=line><span class=cl>&gt;&gt;&gt; Install complete. Run <span class=s2>&#34;ollama&#34;</span> from the <span class=nb>command</span> line.
</span></span><span class=line><span class=cl>WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.
</span></span></code></pre></td></tr></table></div></div><p>Eh, what&rsquo;s now?
For sure I need to uninstall what has been done using the <a href=https://github.com/ollama/ollama/blob/main/docs/linux.md#uninstall>guide</a>
and let&rsquo;s search for another solution.</p><h2 id=intel-ipex-llm>Intel Ipex llm<a hidden class=anchor aria-hidden=true href=#intel-ipex-llm>#</a></h2><p>After quick Googling, I spotted some project on Github that might be interesting
and can &ldquo;fit&rdquo; my requirement - it was Intel Ipex llm available <a href=https://github.com/intel/ipex-llm>here</a>.
Of course there are many projects in Google, that are possitioned higher than
this one - right now AI is very hot topic.</p><h3 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h3><p>There is no rpm package available (I&rsquo;m using Fedora 41), so I will run
the binary directly after unpacking the archive.</p><p>Download the archive from Github project release <a href=https://github.com/ipex-llm/ipex-llm/releases/>page</a> - Ipex LLM:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -LO https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ollama-ipex-llm-2.3.0b20250415-ubuntu.tgz
</span></span></code></pre></td></tr></table></div></div><p>After download, unpack:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>tar xaf ollama-ipex-llm-2.3.0b20250415-ubuntu.tgz
</span></span></code></pre></td></tr></table></div></div><p>Then go to the directly and start the <code>Ollama server</code> by executing:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>./ollama serve
</span></span></code></pre></td></tr></table></div></div><p><img alt=ollama-codellama loading=lazy src=../../posts/images/04-oolama-serve.jpg></p><p>For making simply test for some programming question, I will use a <code>codellama:13b</code> model
available <a href=https://ollama.com/library/codellama:13b>here</a>.
More models with the description are available in the catalog <a href="https://ollama.com/library?sort=popular">page</a>.</p><p><img alt=ollama-codellama loading=lazy src=../../posts/images/05-oolama-codellama.jpg></p><p>That&rsquo;s all for that blog post. In next post I will try to show how the codellama works
with simply queries.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://danpawlik.github.io/tags/first/>First</a></li></ul><nav class=paginav><a class=prev href=https://danpawlik.github.io/posts/intel-npu-driver-disappointment/><span class=title>« Prev</span><br><span>Intel Npu Driver Disappointment</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on x" href="https://x.com/intent/tweet/?text=Running%20Ollama%20on%20Intel%20Arc&amp;url=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f&amp;hashtags=first"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f&amp;title=Running%20Ollama%20on%20Intel%20Arc&amp;summary=Running%20Ollama%20on%20Intel%20Arc&amp;source=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f&title=Running%20Ollama%20on%20Intel%20Arc"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on whatsapp" href="https://api.whatsapp.com/send?text=Running%20Ollama%20on%20Intel%20Arc%20-%20https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on telegram" href="https://telegram.me/share/url?text=Running%20Ollama%20on%20Intel%20Arc&amp;url=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Running Ollama on Intel Arc on ycombinator" href="https://news.ycombinator.com/submitlink?t=Running%20Ollama%20on%20Intel%20Arc&u=https%3a%2f%2fdanpawlik.github.io%2fposts%2follama%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://danpawlik.github.io/>Dan Pawlik blog site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>