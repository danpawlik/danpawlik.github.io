<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using Ollama | Dan Pawlik blog site</title>
<meta name=keywords content="first"><meta name=description content="Desc Text."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.b210b30f6d6919fdd4ea762b4c0a1ecbb0b9e321b761143a2b02b57fe9314f8f.css integrity="sha256-shCzD21pGf3U6nYrTAoey7C54yG3YRQ6KwK1f+kxT48=" rel="preload stylesheet" as=style><link rel=icon href=https://danpawlik.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://danpawlik.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://danpawlik.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://danpawlik.github.io/apple-touch-icon.png><link rel=mask-icon href=https://danpawlik.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://danpawlik.github.io/posts/using-ollama/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://danpawlik.github.io/posts/using-ollama/"><meta property="og:site_name" content="Dan Pawlik blog site"><meta property="og:title" content="Using Ollama"><meta property="og:description" content="Desc Text."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-02T11:01:10+02:00"><meta property="article:modified_time" content="2025-06-02T11:01:10+02:00"><meta property="article:tag" content="First"><meta property="og:image" content="https://danpawlik.github.io/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://danpawlik.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Using Ollama"><meta name=twitter:description content="Desc Text."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://danpawlik.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Using Ollama","item":"https://danpawlik.github.io/posts/using-ollama/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using Ollama","name":"Using Ollama","description":"Desc Text.","keywords":["first"],"articleBody":"Introduction In this blog post, I would try to introduce you, how simply use the Ollama.\nPre requirements For making this exercise, please make sure you have:\nconfigured properly the required packages - you can read about it blog chapter Start Ollama on Fedora and create Python script The Ollama tool can be called using many programming languages. In my example, I would be using Python language for making queries for codellama:13b model.\nFirst, let’s serve the Ollama\nEnsure, that the model is available:\nInstall the ollama package using command:\n1 pip3 install ollama Then create a simply script, for example:\n1 2 3 4 5 6 7 8 9 10 11 cat \u003c\u003c EOF \u003e test.py #!/usr/bin/env python3 import ollama response = ollama.generate( model=\"codellama:13b\", prompt=\"Create script in python that will ask for two numbers then it would add\" ) print(response['response']) Then execute it. In my case, it finished with ERROR:\nWhere Ollama console shows:\nThe exception does not say anything interesting.\nChecking exception error After quick googling, it seems that if error is related to ggml-cpu.c:13612: fatal error: it can be:\nit is normal error from Ollama ggml library and it is a problem on model execution - not enough memory, inproper hardware or library issue I have enough memory (in the logs, I can see that 51G is free), so issue needs to be somewhere else. Another suggestion were:\nno enough VRAM for GPU (NPU) no GPU acceleration - flash_attn = 0 (no Flash Attention) and llama_kv_cache_init: offload = 1 suggest, that the model uses CPU so if I have GPU, it might be unproperly configured ollama version is too new (buggy) version is focused on Ubuntu, might not be work properly on other system From the all suggestions, I think error might be related to all of them. First, let’s assume that the binaries should be executed on Ubuntu system - maybe there is an altnernative way to run the libraries inside the container?\nStart Ollama using container and execute the Python script The container solution might be worth to try - if Intel is saying, that it should be working, but the all libraries are done for Ubuntu - let’s try with Ubuntu.\nBased on the documentation.\nCreating image and container To create image container, execute:\n1 2 3 4 5 6 7 sudo yum install -y podman git git clone https://github.com/intel/ipex-llm \u0026\u0026 cr ipex-llm/docker/llm/inference-cpp # NOTE: the container would be executes with sudo, so to have that # image as sudo, run below command as it is. sudo podman build -t intelanalytics/ipex-llm-inference-cpp-xpu:latest . When all is done, we can start the container:\n1 2 3 4 5 6 7 8 9 10 11 12 mkdir -p $HOME/.ollama sudo podman run -itd \\ --net=host \\ --device=/dev/dri \\ -v $(realpath $HOME)/.ollama:/root/.ollama:z \\ --memory=\"32G\" \\ --name=ipex-llm-inference-cpp-xpu-container \\ -e bench_model=\"mistral-7b-v0.1.Q4_0.gguf\" \\ -e DEVICE=Arc \\ --shm-size=\"16g\" \\ intelanalytics/ipex-llm-inference-cpp-xpu:latest The container should be up and running:\nOf course the benchmark finished with error - need to pull that image first :) Let’s run Ollama, then pull the image.\nNOTE: all below commands are inside the container:\n1 /llm/scripts/start-ollama.sh The “server” should be running in background, so we can type command:\n1 2 # skip that command :) /llm/ollama/ollama pull mistral:7b-instruct-q4_0 Now after pulling image, benchmark should be available. Let’s try again:\n1 2 ln -s ~/.ollama/models /models bash /llm/scripts/benchmark_llama-cpp.sh Still it does not work. Quick Googling and it goes to here…\n1 2 3 4 5 6 7 8 9 10 11 pip3 install huggingface-hub cd /models huggingface-cli download \\ TheBloke/Mistral-7B-v0.1-GGUF \\ mistral-7b-v0.1.Q4_K_M.gguf \\ --local-dir . \\ --local-dir-use-symlinks False mv /models/mistral-7b-v0.1.Q4_K_M.gguf /models/mistral-7b-v0.1.Q4_0.gguf bash /llm/scripts/benchmark_llama-cpp.sh Where final results:\nTo help others if someone want to compare results, I paste it as a code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 sampler seed: 4017167485 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 1024 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.000 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -\u003e logit-bias -\u003e penalties -\u003e dry -\u003e top-k -\u003e typical -\u003e top-p -\u003e min-p -\u003e xtc -\u003e temp-ext -\u003e dist generate: n_ctx = 1024, n_batch = 4096, n_predict = 128, n_keep = 1 It is done, and submitted. You can play 'Survival of the Tastiest' on Android, and on the web. Playing on the web works, but you have to simulate multiple touch for table moving and that can be a bit confusing. There is a lot I'd like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise - something with a lot of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident that I could fit any theme around it. In the end, the problem with a theme like 'Evolution' in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game? In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it's not evolution anymore - it's the equivalent of intelligent design, the fable invented by creationists to combat the idea of evolution. Being agnostic and a Pastafarian, that's not something that rubbed me the right way. Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn't want to create an 'intelligent design' simulator and wrongly call it evolution. This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I'd say the only real solution was through the use of artificial selection, somehow. So far, I haven't seen any entry using this at its core gameplay. Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out. My initial idea was to create something where humanity tried to evolve to a next level, but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn't think of compelling (read: serious) mechanics for that. Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg? The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it. Conversations with my inspiring co-worker Roushey (who also created the 'Mechanical Underdogs' signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist - by evolving from a normal dinner table. So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your 'base'. There are 5 other guests at the table, each with their own plate. Your plate can spawn little pieces of pasta. You do so by 'ordering' them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying 'costs', which are debited from your credits (you start with a number of credits). Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps). Your pasta doesn't like other people's pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill. Once a pasta is in vicinity of a plate, it will try to conquer it. If it is successful, it will become the new plate owner. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to llama_perf_sampler_print: sampling time = 3.63 ms / 1137 runs ( 0.00 ms per token, 313309.45 tokens per second) llama_perf_context_print: load time = 6145.64 ms llama_perf_context_print: prompt eval time = 2899.34 ms / 1009 tokens ( 2.87 ms per token, 348.01 tokens per second) llama_perf_context_print: eval time = 10173.23 ms / 127 runs ( 80.10 ms per token, 12.48 tokens per second) llama_perf_context_print: total time = 13086.18 ms / 1136 tokens The benchmark works, does the Python script work too?\nExecuting Python script The script was already mentioned on the beginning of that blog article. I will not paste it again - you will find it. What is worth to mention, you don’t need to create that script inside the container, or even copy it there. When you create the container, you put an argument: --net=host, so exposing the Ollama default port was not needed (11434) - you can check via netstat/ss and it should be available from your local machine (or my laptop :) )\n1 2 $ ss -nltpu | grep 11434 tcp LISTEN 0 4096 127.0.0.1:11434 0.0.0.0:* Let’s run the script.\n1 python3 test.py when executed, inside the container shows logs:\nand the final code is:\n1 2 3 4 5 6 7 8 num1 = input(\"Enter the first number: \") num2 = input(\"Enter the second number: \") result = num1 + num2 print(f\"The result is {result}\") This script will ask the user to enter two numbers, and then it will add them together using the `+` operator. The result of the addition will be printed to the console. Don’t think it is a proper code, but, maybe other models would understand my queries in better way.\nSummary The Ollama works fine inside the container. Uff, good to have a way to run somehow that tool. Finally it works \\o/ I can make an AI exploration.\nAbout the results - I don’t know if such results are satisfying me, but… maybe they are just ok.\n","wordCount":"1929","inLanguage":"en","image":"https://danpawlik.github.io/%3Cimage%20path/url%3E","datePublished":"2025-06-02T11:01:10+02:00","dateModified":"2025-06-02T11:01:10+02:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://danpawlik.github.io/posts/using-ollama/"},"publisher":{"@type":"Organization","name":"Dan Pawlik blog site","logo":{"@type":"ImageObject","url":"https://danpawlik.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://danpawlik.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://danpawlik.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://danpawlik.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://danpawlik.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://danpawlik.github.io title="main page"><span>main page</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://danpawlik.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://danpawlik.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Using Ollama</h1><div class=post-description>Desc Text.</div><div class=post-meta><span title='2025-06-02 11:01:10 +0200 CEST'>June 2, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1929 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/using-ollama.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#pre-requirements>Pre requirements</a></li><li><a href=#start-ollama-on-fedora-and-create-python-script>Start Ollama on Fedora and create Python script</a><ul><li><a href=#checking-exception-error>Checking exception error</a></li></ul></li><li><a href=#start-ollama-using-container-and-execute-the-python-script>Start Ollama using container and execute the Python script</a><ul><li><a href=#creating-image-and-container>Creating image and container</a></li><li><a href=#executing-python-script>Executing Python script</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In this blog post, I would try to introduce you, how simply use the Ollama.</p><h2 id=pre-requirements>Pre requirements<a hidden class=anchor aria-hidden=true href=#pre-requirements>#</a></h2><p>For making this exercise, please make sure you have:</p><ul><li>configured properly the required packages - you can read about it <a href=https://danpawlik.github.io/posts/intel-npu-driver-disappointment/#looking-for-driver-package>blog chapter</a></li></ul><h2 id=start-ollama-on-fedora-and-create-python-script>Start Ollama on Fedora and create Python script<a hidden class=anchor aria-hidden=true href=#start-ollama-on-fedora-and-create-python-script>#</a></h2><p>The <code>Ollama</code> tool can be called using many programming languages. In my example,
I would be using Python language for making queries for <code>codellama:13b</code> model.</p><p>First, let&rsquo;s serve the Ollama</p><p><img alt=serveOllama loading=lazy src=../../posts/images/01-serve-ollama.jpg></p><p>Ensure, that the model is available:</p><p><img alt=pullModel loading=lazy src=../../posts/images/02-pull-model.jpg></p><p>Install the ollama package using command:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>pip3 install ollama
</span></span></code></pre></td></tr></table></div></div><p>Then create a simply script, for example:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cat &lt;&lt; EOF &gt; test.py
</span></span><span class=line><span class=cl><span class=c1>#!/usr/bin/env python3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>import ollama
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>response</span> <span class=o>=</span> ollama.generate<span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=nv>model</span><span class=o>=</span><span class=s2>&#34;codellama:13b&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=nv>prompt</span><span class=o>=</span><span class=s2>&#34;Create script in python that will ask for two numbers then it would add&#34;</span>
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>print<span class=o>(</span>response<span class=o>[</span><span class=s1>&#39;response&#39;</span><span class=o>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Then execute it. In my case, it finished with ERROR:</p><p><img alt=pythonError loading=lazy src=../../posts/images/03-python-error.jpg></p><p>Where Ollama console shows:</p><p><img alt=queryProcessing loading=lazy src=../../posts/images/04-query-processing.jpg></p><p><img alt=queryProcessingContinuation loading=lazy src=../../posts/images/05-query-processing-continuation.jpg></p><p><img alt=gdbDebug loading=lazy src=../../posts/images/06-query-processing-gdb.jpg></p><p><img alt=gdbDebugContinuation loading=lazy src=../../posts/images/06-query-processing-gdb-continuation.jpg></p><p>The exception does not say anything interesting.</p><h3 id=checking-exception-error>Checking exception error<a hidden class=anchor aria-hidden=true href=#checking-exception-error>#</a></h3><p>After quick googling, it seems that if error is related to <code>ggml-cpu.c:13612: fatal error:</code>
it can be:</p><ul><li>it is normal error from Ollama ggml library and</li><li>it is a problem on model execution - not enough memory, inproper hardware or
library issue</li></ul><p>I have enough memory (in the logs, I can see that 51G is free), so issue needs to
be somewhere else. Another suggestion were:</p><ul><li>no enough VRAM for GPU (NPU)</li><li>no GPU acceleration - <code>flash_attn = 0</code> (no Flash Attention) and <code>llama_kv_cache_init: offload = 1</code>
suggest, that the model uses CPU so if I have GPU, it might be unproperly configured</li><li>ollama version is too new (buggy)</li><li>version is focused on Ubuntu, might not be work properly on other system</li></ul><p>From the all suggestions, I think error might be related to all of them.
First, let&rsquo;s assume that the binaries should be executed on <code>Ubuntu</code> system -
maybe there is an altnernative way to run the libraries inside the container?</p><h2 id=start-ollama-using-container-and-execute-the-python-script>Start Ollama using container and execute the Python script<a hidden class=anchor aria-hidden=true href=#start-ollama-using-container-and-execute-the-python-script>#</a></h2><p>The container solution might be worth to try - if Intel is saying, that it should
be working, but the all libraries are done for Ubuntu - let&rsquo;s try with Ubuntu.</p><p>Based on the <a href=https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md>documentation</a>.</p><h3 id=creating-image-and-container>Creating image and container<a hidden class=anchor aria-hidden=true href=#creating-image-and-container>#</a></h3><p>To create image container, execute:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4>4</a>
</span><span class=lnt id=hl-2-5><a class=lnlinks href=#hl-2-5>5</a>
</span><span class=lnt id=hl-2-6><a class=lnlinks href=#hl-2-6>6</a>
</span><span class=lnt id=hl-2-7><a class=lnlinks href=#hl-2-7>7</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sudo yum install -y podman git
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>git clone https://github.com/intel/ipex-llm <span class=o>&amp;&amp;</span> cr ipex-llm/docker/llm/inference-cpp
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># NOTE: the container would be executes with sudo, so to have that</span>
</span></span><span class=line><span class=cl><span class=c1># image as sudo, run below command as it is.</span>
</span></span><span class=line><span class=cl>sudo podman build -t intelanalytics/ipex-llm-inference-cpp-xpu:latest .
</span></span></code></pre></td></tr></table></div></div><p>When all is done, we can start the container:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1> 1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2> 2</a>
</span><span class=lnt id=hl-3-3><a class=lnlinks href=#hl-3-3> 3</a>
</span><span class=lnt id=hl-3-4><a class=lnlinks href=#hl-3-4> 4</a>
</span><span class=lnt id=hl-3-5><a class=lnlinks href=#hl-3-5> 5</a>
</span><span class=lnt id=hl-3-6><a class=lnlinks href=#hl-3-6> 6</a>
</span><span class=lnt id=hl-3-7><a class=lnlinks href=#hl-3-7> 7</a>
</span><span class=lnt id=hl-3-8><a class=lnlinks href=#hl-3-8> 8</a>
</span><span class=lnt id=hl-3-9><a class=lnlinks href=#hl-3-9> 9</a>
</span><span class=lnt id=hl-3-10><a class=lnlinks href=#hl-3-10>10</a>
</span><span class=lnt id=hl-3-11><a class=lnlinks href=#hl-3-11>11</a>
</span><span class=lnt id=hl-3-12><a class=lnlinks href=#hl-3-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mkdir -p <span class=nv>$HOME</span>/.ollama
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>sudo podman run -itd <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --net<span class=o>=</span>host <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --device<span class=o>=</span>/dev/dri <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -v <span class=k>$(</span>realpath <span class=nv>$HOME</span><span class=k>)</span>/.ollama:/root/.ollama:z <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --memory<span class=o>=</span><span class=s2>&#34;32G&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --name<span class=o>=</span>ipex-llm-inference-cpp-xpu-container <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -e <span class=nv>bench_model</span><span class=o>=</span><span class=s2>&#34;mistral-7b-v0.1.Q4_0.gguf&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -e <span class=nv>DEVICE</span><span class=o>=</span>Arc <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --shm-size<span class=o>=</span><span class=s2>&#34;16g&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    intelanalytics/ipex-llm-inference-cpp-xpu:latest
</span></span></code></pre></td></tr></table></div></div><p>The container should be up and running:</p><p><img alt=podmanContainer loading=lazy src=../../posts/images/08-podman-container.jpg></p><p>Of course the benchmark finished with error - need to pull that image first :)
Let&rsquo;s run Ollama, then pull the image.</p><p>NOTE: all below commands are inside the container:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>/llm/scripts/start-ollama.sh
</span></span></code></pre></td></tr></table></div></div><p>The &ldquo;server&rdquo; should be running in background, so we can type command:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1>1</a>
</span><span class=lnt id=hl-5-2><a class=lnlinks href=#hl-5-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># skip that command :)</span>
</span></span><span class=line><span class=cl>/llm/ollama/ollama pull mistral:7b-instruct-q4_0
</span></span></code></pre></td></tr></table></div></div><p><img alt=pullImage loading=lazy src=../../posts/images/09-ollamaPull.jpg></p><p>Now after pulling image, benchmark should be available. Let&rsquo;s try again:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1>1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ln -s ~/.ollama/models /models
</span></span><span class=line><span class=cl>bash /llm/scripts/benchmark_llama-cpp.sh
</span></span></code></pre></td></tr></table></div></div><p>Still it does not work. Quick Googling and it goes to <a href=https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF>here</a>&mldr;</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-7-1><a class=lnlinks href=#hl-7-1> 1</a>
</span><span class=lnt id=hl-7-2><a class=lnlinks href=#hl-7-2> 2</a>
</span><span class=lnt id=hl-7-3><a class=lnlinks href=#hl-7-3> 3</a>
</span><span class=lnt id=hl-7-4><a class=lnlinks href=#hl-7-4> 4</a>
</span><span class=lnt id=hl-7-5><a class=lnlinks href=#hl-7-5> 5</a>
</span><span class=lnt id=hl-7-6><a class=lnlinks href=#hl-7-6> 6</a>
</span><span class=lnt id=hl-7-7><a class=lnlinks href=#hl-7-7> 7</a>
</span><span class=lnt id=hl-7-8><a class=lnlinks href=#hl-7-8> 8</a>
</span><span class=lnt id=hl-7-9><a class=lnlinks href=#hl-7-9> 9</a>
</span><span class=lnt id=hl-7-10><a class=lnlinks href=#hl-7-10>10</a>
</span><span class=lnt id=hl-7-11><a class=lnlinks href=#hl-7-11>11</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>pip3 install huggingface-hub
</span></span><span class=line><span class=cl><span class=nb>cd</span> /models
</span></span><span class=line><span class=cl>huggingface-cli download <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    TheBloke/Mistral-7B-v0.1-GGUF <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    mistral-7b-v0.1.Q4_K_M.gguf <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --local-dir . <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --local-dir-use-symlinks False
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>mv /models/mistral-7b-v0.1.Q4_K_M.gguf /models/mistral-7b-v0.1.Q4_0.gguf
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash /llm/scripts/benchmark_llama-cpp.sh
</span></span></code></pre></td></tr></table></div></div><p><img alt=benchmark loading=lazy src=../../posts/images/10-benchmark.jpg></p><p>Where final results:</p><p><img alt=benchmarkResults loading=lazy src=../../posts/images/11-benchmark-results.jpg></p><p>To help others if someone want to compare results, I paste it as a code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-8-1><a class=lnlinks href=#hl-8-1> 1</a>
</span><span class=lnt id=hl-8-2><a class=lnlinks href=#hl-8-2> 2</a>
</span><span class=lnt id=hl-8-3><a class=lnlinks href=#hl-8-3> 3</a>
</span><span class=lnt id=hl-8-4><a class=lnlinks href=#hl-8-4> 4</a>
</span><span class=lnt id=hl-8-5><a class=lnlinks href=#hl-8-5> 5</a>
</span><span class=lnt id=hl-8-6><a class=lnlinks href=#hl-8-6> 6</a>
</span><span class=lnt id=hl-8-7><a class=lnlinks href=#hl-8-7> 7</a>
</span><span class=lnt id=hl-8-8><a class=lnlinks href=#hl-8-8> 8</a>
</span><span class=lnt id=hl-8-9><a class=lnlinks href=#hl-8-9> 9</a>
</span><span class=lnt id=hl-8-10><a class=lnlinks href=#hl-8-10>10</a>
</span><span class=lnt id=hl-8-11><a class=lnlinks href=#hl-8-11>11</a>
</span><span class=lnt id=hl-8-12><a class=lnlinks href=#hl-8-12>12</a>
</span><span class=lnt id=hl-8-13><a class=lnlinks href=#hl-8-13>13</a>
</span><span class=lnt id=hl-8-14><a class=lnlinks href=#hl-8-14>14</a>
</span><span class=lnt id=hl-8-15><a class=lnlinks href=#hl-8-15>15</a>
</span><span class=lnt id=hl-8-16><a class=lnlinks href=#hl-8-16>16</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sampler seed: <span class=m>4017167485</span>
</span></span><span class=line><span class=cl>sampler params:
</span></span><span class=line><span class=cl>        <span class=nv>repeat_last_n</span> <span class=o>=</span> 64, <span class=nv>repeat_penalty</span> <span class=o>=</span> 1.000, <span class=nv>frequency_penalty</span> <span class=o>=</span> 0.000, <span class=nv>presence_penalty</span> <span class=o>=</span> 0.000
</span></span><span class=line><span class=cl>        <span class=nv>dry_multiplier</span> <span class=o>=</span> 0.000, <span class=nv>dry_base</span> <span class=o>=</span> 1.750, <span class=nv>dry_allowed_length</span> <span class=o>=</span> 2, <span class=nv>dry_penalty_last_n</span> <span class=o>=</span> <span class=m>1024</span>
</span></span><span class=line><span class=cl>        <span class=nv>top_k</span> <span class=o>=</span> 40, <span class=nv>top_p</span> <span class=o>=</span> 0.950, <span class=nv>min_p</span> <span class=o>=</span> 0.050, <span class=nv>xtc_probability</span> <span class=o>=</span> 0.000, <span class=nv>xtc_threshold</span> <span class=o>=</span> 0.100, <span class=nv>typical_p</span> <span class=o>=</span> 1.000, <span class=nv>top_n_sigma</span> <span class=o>=</span> -1.000, <span class=nv>temp</span> <span class=o>=</span> 0.000
</span></span><span class=line><span class=cl>        <span class=nv>mirostat</span> <span class=o>=</span> 0, <span class=nv>mirostat_lr</span> <span class=o>=</span> 0.100, <span class=nv>mirostat_ent</span> <span class=o>=</span> 5.000
</span></span><span class=line><span class=cl>sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist
</span></span><span class=line><span class=cl>generate: <span class=nv>n_ctx</span> <span class=o>=</span> 1024, <span class=nv>n_batch</span> <span class=o>=</span> 4096, <span class=nv>n_predict</span> <span class=o>=</span> 128, <span class=nv>n_keep</span> <span class=o>=</span> <span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> It is <span class=k>done</span>, and submitted. You can play <span class=s1>&#39;Survival of the Tastiest&#39;</span> on Android, and on the web. Playing on the web works, but you have to simulate multiple touch <span class=k>for</span> table moving and that can be a bit confusing. There is a lot I<span class=s1>&#39;d like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise - something with a lot of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident that I could fit any theme around it. In the end, the problem with a theme like &#39;</span>Evolution<span class=s1>&#39; in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game? In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it&#39;</span>s not evolution anymore - it<span class=s1>&#39;s the equivalent of intelligent design, the fable invented by creationists to combat the idea of evolution. Being agnostic and a Pastafarian, that&#39;</span>s not something that rubbed me the right way. Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn<span class=s1>&#39;t want to create an &#39;</span>intelligent design<span class=s1>&#39; simulator and wrongly call it evolution. This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I&#39;</span>d say the only real solution was through the use of artificial selection, somehow. So far, I haven<span class=s1>&#39;t seen any entry using this at its core gameplay. Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out. My initial idea was to create something where humanity tried to evolve to a next level, but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn&#39;</span>t think of compelling <span class=o>(</span>read: serious<span class=o>)</span> mechanics <span class=k>for</span> that. Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg? The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it. Conversations with my inspiring co-worker Roushey <span class=o>(</span>who also created the <span class=s1>&#39;Mechanical Underdogs&#39;</span> signature logo <span class=k>for</span> my intros<span class=o>)</span> further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve <span class=k>until</span> they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist - by evolving from a normal dinner table. So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your <span class=s1>&#39;base&#39;</span>. There are <span class=m>5</span> other guests at the table, each with their own plate. Your plate can spawn little pieces of pasta. You <span class=k>do</span> so by <span class=s1>&#39;ordering&#39;</span> them through a menu. Some pastas are better than others<span class=p>;</span> some are faster, some are stronger. They have varying <span class=s1>&#39;costs&#39;</span>, which are debited from your credits <span class=o>(</span>you start with a number of credits<span class=o>)</span>. Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them <span class=o>(</span>the objective of the game is having your pasta conquer all the plates on the table<span class=o>)</span>. But they are really autonomous, so after being spawned, you have no control over your pasta <span class=o>(</span>think DotA or LoL creeps<span class=o>)</span>. Your pasta doesn<span class=s1>&#39;t like other people&#39;</span>s pasta, so <span class=k>if</span> they meet, they shoot sauce at each other <span class=k>until</span> one dies. You get credits <span class=k>for</span> other pastas your own pasta kill. Once a pasta is in vicinity of a plate, it will try to conquer it. If it is successful, it will become the new plate owner. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to conquer the plate again. If it is unsuccessful, it will try to
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llama_perf_sampler_print:    sampling <span class=nb>time</span> <span class=o>=</span>       3.63 ms /  <span class=m>1137</span> runs   <span class=o>(</span>    0.00 ms per token, 313309.45 tokens per second<span class=o>)</span>
</span></span><span class=line><span class=cl>llama_perf_context_print:        load <span class=nb>time</span> <span class=o>=</span>    6145.64 ms
</span></span><span class=line><span class=cl>llama_perf_context_print: prompt <span class=nb>eval</span> <span class=nb>time</span> <span class=o>=</span>    2899.34 ms /  <span class=m>1009</span> tokens <span class=o>(</span>    2.87 ms per token,   348.01 tokens per second<span class=o>)</span>
</span></span><span class=line><span class=cl>llama_perf_context_print:        <span class=nb>eval</span> <span class=nb>time</span> <span class=o>=</span>   10173.23 ms /   <span class=m>127</span> runs   <span class=o>(</span>   80.10 ms per token,    12.48 tokens per second<span class=o>)</span>
</span></span><span class=line><span class=cl>llama_perf_context_print:       total <span class=nb>time</span> <span class=o>=</span>   13086.18 ms /  <span class=m>1136</span> tokens
</span></span></code></pre></td></tr></table></div></div><p>The benchmark works, does the Python script work too?</p><h3 id=executing-python-script>Executing Python script<a hidden class=anchor aria-hidden=true href=#executing-python-script>#</a></h3><p>The script was already mentioned on the beginning of that blog article.
I will not paste it again - you will find it.
What is worth to mention, you don&rsquo;t need to create that script inside the container,
or even copy it there. When you create the container, you put an argument: <code>--net=host</code>,
so exposing the <code>Ollama</code> default port was not needed (<code>11434</code>) - you can check via
netstat/ss and it should be available from your local machine (or my laptop :) )</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-9-1><a class=lnlinks href=#hl-9-1>1</a>
</span><span class=lnt id=hl-9-2><a class=lnlinks href=#hl-9-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ ss -nltpu <span class=p>|</span> grep <span class=m>11434</span>
</span></span><span class=line><span class=cl>tcp   LISTEN <span class=m>0</span>      <span class=m>4096</span>       127.0.0.1:11434      0.0.0.0:*
</span></span></code></pre></td></tr></table></div></div><p>Let&rsquo;s run the script.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-10-1><a class=lnlinks href=#hl-10-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>python3 test.py
</span></span></code></pre></td></tr></table></div></div><p>when executed, inside the container shows logs:</p><p><img alt=ollamaLogs loading=lazy src=../../posts/images/12-ollama-logs.jpg></p><p>and the final code is:</p><p><img alt=finalCode loading=lazy src=../../posts/images/13-final-code.jpg></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-11-1><a class=lnlinks href=#hl-11-1>1</a>
</span><span class=lnt id=hl-11-2><a class=lnlinks href=#hl-11-2>2</a>
</span><span class=lnt id=hl-11-3><a class=lnlinks href=#hl-11-3>3</a>
</span><span class=lnt id=hl-11-4><a class=lnlinks href=#hl-11-4>4</a>
</span><span class=lnt id=hl-11-5><a class=lnlinks href=#hl-11-5>5</a>
</span><span class=lnt id=hl-11-6><a class=lnlinks href=#hl-11-6>6</a>
</span><span class=lnt id=hl-11-7><a class=lnlinks href=#hl-11-7>7</a>
</span><span class=lnt id=hl-11-8><a class=lnlinks href=#hl-11-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nv>num1</span> <span class=o>=</span> input<span class=o>(</span><span class=s2>&#34;Enter the first number: &#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>num2</span> <span class=o>=</span> input<span class=o>(</span><span class=s2>&#34;Enter the second number: &#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>result</span> <span class=o>=</span> num1 + num2
</span></span><span class=line><span class=cl>print<span class=o>(</span>f<span class=s2>&#34;The result is {result}&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>This script will ask the user to enter two numbers, and <span class=k>then</span> it will add them
</span></span><span class=line><span class=cl>together using the <span class=sb>`</span>+<span class=sb>`</span> operator. The result of the addition will be printed
</span></span><span class=line><span class=cl>to the console.
</span></span></code></pre></td></tr></table></div></div><p>Don&rsquo;t think it is a proper code, but, maybe other models would understand
my queries in better way.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>The Ollama works fine inside the container. Uff, good to have a way to run somehow
that tool. Finally it works \o/ I can make an AI exploration.</p><p>About the results - I don&rsquo;t know if such results are satisfying me, but&mldr; maybe they are just ok.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://danpawlik.github.io/tags/first/>First</a></li></ul><nav class=paginav><a class=next href=https://danpawlik.github.io/posts/intel-npu-driver-disappointment/><span class=title>Next »</span><br><span>Intel Npu Driver Disappointment</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on x" href="https://x.com/intent/tweet/?text=Using%20Ollama&amp;url=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f&amp;hashtags=first"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f&amp;title=Using%20Ollama&amp;summary=Using%20Ollama&amp;source=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f&title=Using%20Ollama"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on whatsapp" href="https://api.whatsapp.com/send?text=Using%20Ollama%20-%20https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on telegram" href="https://telegram.me/share/url?text=Using%20Ollama&amp;url=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using Ollama on ycombinator" href="https://news.ycombinator.com/submitlink?t=Using%20Ollama&u=https%3a%2f%2fdanpawlik.github.io%2fposts%2fusing-ollama%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://danpawlik.github.io/>Dan Pawlik blog site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>